{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Physics Informed Neural Network!**\n",
        "\n",
        "**Disclaimer**: large parts of the lab are taken from [this github repository](https://github.com/nanditadoloi/PINN/blob/main/solve_PDE_NN.ipynb) and [this one](https://github.com/jayroxis/PINNs) both based on the original work of [Raissi, Perdikaris and Karniadakis](https://doi.org/10.1016/j.jcp.2018.10.045)."
      ],
      "metadata": {
        "id": "mbMZU3O8kHoC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Definition**\n",
        "\n",
        "Physics Informed Neural Netwoks (PINN) relates deep learning and Partial Differential Equations (PDEs). The main idea is not to purely rely on data, but to exploit the physical model (i.e. the _residual_ of the PDE) in the loss function.\n",
        "\n",
        "**Pro**: the approach is totally mesh-free.\n",
        "\n",
        "**Cons**: difficult to train for complicated problems."
      ],
      "metadata": {
        "id": "kk47Rpda0N8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The loss**\n",
        "Let us be more specific:\n",
        "\n",
        "The loss function of a PINN is made by: \n",
        "* boundary information\n",
        "* residual information \n",
        "\n",
        "Let us suppose that the physical system we are studying is described by a PDE of the form\n",
        "$\n",
        "\\mathcal R({w}(\\mathbf x)) = 0\n",
        "$\n",
        "\n",
        "Namely, we want to fine a surrogate Neural Network $\\tilde{w}$ that minimizes:\n",
        "\n",
        "$$\n",
        "    MSE  \\doteq MSE_b + MSE_p.\n",
        "$$\n",
        "where\n",
        "$$ \n",
        "        MSE_b \\doteq \\frac{1}{N_b} \\sum_{k=1}^{N_b} | \\tilde{w}(\\mathbf x_k^b) - w(\\mathbf x_k^b)|^2,\n",
        "$$\n",
        "for $\\mathbf x_k^b \\in \\partial \\Omega$. Here, $MSE_b$ is the _boundary_ loss. While \n",
        "    \\begin{equation*}\n",
        "        MSE_{p} \\doteq \\frac{1}{N_p}\\sum_{k=1}^{N_p}|\\mathcal R(\\tilde{w}(\\mathbf x_k^p))|^2.\n",
        "    \\end{equation*}\n",
        "\n",
        "is the _physical_ (or residual) loss for $\\mathbf x_k^p \\in \\Omega$."
      ],
      "metadata": {
        "id": "wdoLL_zPB_zo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **A first Example: the 1D heat equation**\n",
        "\n",
        "The physical model is given in the space-time domain $[0,2]\\times[0,1]$: \n",
        "\n",
        "$$\\frac{ du}{ dx} = \\frac{2du}{dt}+u$$ \n",
        "\n",
        "with the following boundary condition $u(x,0)=6e^{-3x}$\n",
        "\n",
        "Independent variables: x,t (input)\n",
        "Dependent variables: u (outputs)\n",
        "We have to find out u(x,t) for all x in range [0,2] and t in range [0,1]\n",
        "\n",
        "Analytically, the solution is  $u(x,t) = 6e^{(-3x-2t)}$."
      ],
      "metadata": {
        "id": "_fplIfaMYRnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### starting stuff ####\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "4Iw4aGhrkGOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let uf define the Neural Network!\n",
        "\n",
        "**_Question time!_** what are the input and output dimension?"
      ],
      "metadata": {
        "id": "beo5QVoIdauv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.input_layer = nn.Linear(2,5)\n",
        "        self.hidden_layer1 = nn.Linear(5,5)\n",
        "        self.hidden_layer2 = nn.Linear(5,5)\n",
        "        self.hidden_layer3 = nn.Linear(5,5)\n",
        "        self.hidden_layer4 = nn.Linear(5,5)\n",
        "        self.output_layer = nn.Linear(5,1)\n",
        "\n",
        "    def forward(self, x,t):\n",
        "        input = torch.cat([x,t],axis=1) # combines the column array\n",
        "        layer1_out = torch.sigmoid(self.input_layer(input))\n",
        "        layer2_out = torch.sigmoid(self.hidden_layer1(layer1_out))\n",
        "        layer3_out = torch.sigmoid(self.hidden_layer2(layer2_out))\n",
        "        layer4_out = torch.sigmoid(self.hidden_layer3(layer3_out))\n",
        "        layer5_out = torch.sigmoid(self.hidden_layer4(layer4_out))\n",
        "        output = self.output_layer(layer5_out)\n",
        "        return output"
      ],
      "metadata": {
        "id": "00R614vvZyDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### (2) Model\n",
        "net = Net()\n",
        "mse_cost_function = torch.nn.MSELoss() # Mean squared error\n",
        "optimizer = torch.optim.Adam(net.parameters())"
      ],
      "metadata": {
        "id": "FrTxOzTRddwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## PDE as loss function. Thus would use the network which we call as u_theta\n",
        "def R(x,t, net):\n",
        "    u = net(x,t) # the dependent variable u is given by the network based on independent variables x,t\n",
        "    ## Based on our R = du/dx - 2du/dt - u, we need du/dx and du/dt\n",
        "    u_x = torch.autograd.grad(u.sum(), x, create_graph=True)[0]\n",
        "    u_t = torch.autograd.grad(u.sum(), t, create_graph=True)[0]\n",
        "    pde = u_x - 2*u_t - u\n",
        "    return pde"
      ],
      "metadata": {
        "id": "e3SU7jgZehM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Data from Boundary Conditions\n",
        "# u(x,0)=6e^(-3x)\n",
        "## BC just gives us datapoints for training\n",
        "\n",
        "# BC tells us that for any x in range[0,2] and time=0, the value of u is given by 6e^(-3x)\n",
        "# Take say 500 random numbers of x\n",
        "x_bc = np.random.uniform(low=0.0, high=2.0, size=(500,1))\n",
        "t_bc = np.zeros((500,1))\n",
        "# compute u based on BC\n",
        "u_bc = 6*np.exp(-3*x_bc)"
      ],
      "metadata": {
        "id": "Bu_G_KUYe-yO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### (3) Training / Fitting\n",
        "iterations = 10000\n",
        "for epoch in range(iterations):\n",
        "    optimizer.zero_grad() # to make the gradients zero\n",
        "    \n",
        "    # Loss based on boundary conditions\n",
        "    pt_x_bc = Variable(torch.from_numpy(x_bc).float(), requires_grad=False)\n",
        "    pt_t_bc = Variable(torch.from_numpy(t_bc).float(), requires_grad=False)\n",
        "    pt_u_bc = Variable(torch.from_numpy(u_bc).float(), requires_grad=False)\n",
        "    \n",
        "    net_bc_out = net(pt_x_bc, pt_t_bc) # output of u(x,t)\n",
        "    mse_u = mse_cost_function(net_bc_out, pt_u_bc)\n",
        "    \n",
        "    # Loss based on PDE\n",
        "    x_collocation = np.random.uniform(low=0.0, high=2.0, size=(500,1))\n",
        "    t_collocation = np.random.uniform(low=0.0, high=1.0, size=(500,1))\n",
        "    all_zeros = np.zeros((500,1))\n",
        "    \n",
        "    \n",
        "    pt_x_collocation = Variable(torch.from_numpy(x_collocation).float(), requires_grad=True)\n",
        "    pt_t_collocation = Variable(torch.from_numpy(t_collocation).float(), requires_grad=True)\n",
        "    pt_all_zeros = Variable(torch.from_numpy(all_zeros).float(), requires_grad=False)\n",
        "    \n",
        "    f_out = R(pt_x_collocation, pt_t_collocation, net) # output of R(x,t)\n",
        "    mse_f = mse_cost_function(f_out, pt_all_zeros)\n",
        "    \n",
        "    # Combining the loss functions\n",
        "    loss = mse_u + mse_f\n",
        "    \n",
        "    \n",
        "    loss.backward() \n",
        "    optimizer.step()\n",
        "\n",
        "    with torch.autograd.no_grad():\n",
        "    \tprint(epoch,\"Loss:\",loss.item())"
      ],
      "metadata": {
        "id": "l0dSc1hffFaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
        "import numpy as np\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.gca(projection='3d')\n",
        "\n",
        "x=np.arange(0,2,0.02)\n",
        "t=np.arange(0,1,0.02)\n",
        "ms_x, ms_t = np.meshgrid(x, t)\n",
        "## Just because meshgrid is used, we need to do the following adjustment\n",
        "x = np.ravel(ms_x).reshape(-1,1)\n",
        "t = np.ravel(ms_t).reshape(-1,1)\n",
        "\n",
        "pt_x = Variable(torch.from_numpy(x).float(), requires_grad=True)\n",
        "pt_t = Variable(torch.from_numpy(t).float(), requires_grad=True)\n",
        "pt_u = net(pt_x,pt_t)\n",
        "u=pt_u.data.cpu().numpy()\n",
        "ms_u = u.reshape(ms_x.shape)\n",
        "\n",
        "surf = ax.plot_surface(ms_x,ms_t,ms_u, cmap=cm.coolwarm,linewidth=0, antialiased=False)\n",
        "             \n",
        "             \n",
        "\n",
        "\n",
        "\n",
        "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jZql-zdljnV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.gca(projection='3d')\n",
        "\n",
        "pt_x = Variable(torch.from_numpy(x).float(), requires_grad=True).detach().numpy()\n",
        "pt_t = Variable(torch.from_numpy(t).float(), requires_grad=True).detach().numpy()\n",
        "R_u = 6*np.exp(-3*pt_x - 2*pt_t)\n",
        "u=pt_u.data.cpu().numpy()\n",
        "ms_u = u.reshape(ms_x.shape)\n",
        "\n",
        "surf = ax.plot_surface(ms_x,ms_t,ms_u, cmap=cm.coolwarm,linewidth=0, antialiased=False)\n",
        "             \n",
        "             \n",
        "\n",
        "\n",
        "\n",
        "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "34b1oVPCkP7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Your Turn: let's change the equation**\n"
      ],
      "metadata": {
        "id": "-WeirUGMlVtm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take a look to [this github repository](https://github.com/jayroxis/PINNs). The proposed tutorial is a bit more involved. \n",
        "\n",
        "We suggest to study it and to try to understand it line-by-line.\n",
        "\n",
        "It tries to represent the solution of the Burgers equation\n",
        "\n",
        "$$\n",
        "u_t + \\lambda_1 uu_x - \\lambda_2 u_{xx}\n",
        "$$\n",
        "\n",
        "for some fixed parameters $\\lambda_1$ and $\\lambda_2$.\n",
        "\n",
        "Since no analytical solution is present, the code uses data provided by numerical simulations.\n",
        "\n",
        "To use them, copy the data in your google drive and use the following commands:\n",
        "\n",
        "``import scipy.io``\n",
        "\n",
        "``from google.colab import drive``\n",
        "\n",
        "``drive.mount('/content/drive')``\n",
        "\n",
        "``data = scipy.io.loadmat('MyDrive/colab/RBM_ML/data/burgers_shock.mat')``"
      ],
      "metadata": {
        "id": "8pGgZGeha3vS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0, '../Utilities/')\n",
        "\n",
        "import torch\n",
        "from collections import OrderedDict\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "from scipy.interpolate import griddata\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "import matplotlib.gridspec as gridspec\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "np.random.seed(1234)"
      ],
      "metadata": {
        "id": "377d6xiOpmhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the deep neural network\n",
        "class DNN(torch.nn.Module):\n",
        "    def __init__(self, layers):\n",
        "        super(DNN, self).__init__()\n",
        "        \n",
        "        # parameters\n",
        "        self.depth = len(layers) - 1\n",
        "        \n",
        "        # set up layer order dict\n",
        "        self.activation = torch.nn.Tanh\n",
        "        \n",
        "        layer_list = list()\n",
        "        for i in range(self.depth - 1): \n",
        "            layer_list.append(\n",
        "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n",
        "            )\n",
        "            layer_list.append(('activation_%d' % i, self.activation()))\n",
        "            \n",
        "        layer_list.append(\n",
        "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
        "        )\n",
        "        layerDict = OrderedDict(layer_list)\n",
        "        \n",
        "        # deploy layers\n",
        "        self.layers = torch.nn.Sequential(layerDict) ## layer, activation, layer, activation\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.layers(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "-qtk-nVYpp2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the physics-guided neural network\n",
        "class PhysicsInformedNN():\n",
        "    def __init__(self, X, u, layers, lb, ub):\n",
        "        \n",
        "        # boundary conditions\n",
        "        self.lb = torch.tensor(lb).float()\n",
        "        self.ub = torch.tensor(ub).float()\n",
        "        \n",
        "        # data\n",
        "        self.x = torch.tensor(X[:, 0:1], requires_grad=True).float()\n",
        "        self.t = torch.tensor(X[:, 1:2], requires_grad=True).float()\n",
        "        self.u = torch.tensor(u).float()\n",
        "        \n",
        "        # settings\n",
        "        self.lambda_1 = torch.tensor([0.0], requires_grad=True)\n",
        "        self.lambda_2 = torch.tensor([-6.0], requires_grad=True)\n",
        "        \n",
        "        self.lambda_1 = torch.nn.Parameter(self.lambda_1)\n",
        "        self.lambda_2 = torch.nn.Parameter(self.lambda_2)\n",
        "        \n",
        "        # deep neural networks\n",
        "        self.dnn = DNN(layers)\n",
        "        self.dnn.register_parameter('lambda_1', self.lambda_1)\n",
        "        self.dnn.register_parameter('lambda_2', self.lambda_2)\n",
        "        \n",
        "         # optimizers: using the same settings\n",
        "        self.optimizer = torch.optim.LBFGS(\n",
        "            self.dnn.parameters(), \n",
        "            lr=1.0, \n",
        "            max_iter=50000, \n",
        "            max_eval=50000, \n",
        "            history_size=50,\n",
        "            tolerance_grad=1e-5, \n",
        "            tolerance_change=1.0 * np.finfo(float).eps,\n",
        "            line_search_fn=\"strong_wolfe\"       # can be \"strong_wolfe\"\n",
        "        )\n",
        "        \n",
        "        self.optimizer_Adam = torch.optim.Adam(self.dnn.parameters())\n",
        "        self.iter = 0\n",
        "        \n",
        "    def net_u(self, x, t):  \n",
        "        u = self.dnn(torch.cat([x, t], dim=1))\n",
        "        return u\n",
        "    \n",
        "    def net_f(self, x, t):\n",
        "        \"\"\" The pytorch autograd version of calculating residual \"\"\"\n",
        "        lambda_1 = self.lambda_1        \n",
        "        lambda_2 = torch.exp(self.lambda_2)\n",
        "        u = self.net_u(x, t)\n",
        "        \n",
        "        u_t = torch.autograd.grad(\n",
        "            u, t, \n",
        "            grad_outputs=torch.ones_like(u),\n",
        "            retain_graph=True,\n",
        "            create_graph=True\n",
        ")[0]\n",
        "        u_x = torch.autograd.grad(\n",
        "            u, x, \n",
        "            grad_outputs=torch.ones_like(u),\n",
        "            retain_graph=True,\n",
        "            create_graph=True\n",
        "        )[0]\n",
        "        u_xx = torch.autograd.grad(\n",
        "            u_x, x, \n",
        "            grad_outputs=torch.ones_like(u_x),\n",
        "            retain_graph=True,\n",
        "            create_graph=True\n",
        "        )[0]\n",
        "        \n",
        "        f = u_t + lambda_1 * u * u_x - lambda_2 * u_xx\n",
        "        return f\n",
        "    \n",
        "    def loss_func(self):\n",
        "        u_pred = self.net_u(self.x, self.t)\n",
        "        f_pred = self.net_f(self.x, self.t)\n",
        "        loss = torch.mean((self.u - u_pred) ** 2) + torch.mean(f_pred ** 2)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        \n",
        "        self.iter += 1\n",
        "        if self.iter % 100 == 0:\n",
        "            print(\n",
        "                'Loss: %e, l1: %.5f, l2: %.5f' % \n",
        "                (\n",
        "                    loss.item(), \n",
        "                    self.lambda_1.item(), \n",
        "                    torch.exp(self.lambda_2.detach()).item()\n",
        "                )\n",
        "            )\n",
        "        return loss\n",
        "    \n",
        "    def train(self, nIter):\n",
        "        self.dnn.train()\n",
        "        for epoch in range(nIter):\n",
        "            u_pred = self.net_u(self.x, self.t)\n",
        "            f_pred = self.net_f(self.x, self.t)\n",
        "            loss = torch.mean((self.u - u_pred) ** 2) + torch.mean(f_pred ** 2)\n",
        "            \n",
        "            # Backward and optimize\n",
        "            self.optimizer_Adam.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer_Adam.step()\n",
        "            \n",
        "            if epoch % 100 == 0:\n",
        "                print(\n",
        "                    'It: %d, Loss: %.3e, Lambda_1: %.3f, Lambda_2: %.6f' % \n",
        "                    (\n",
        "                        epoch, \n",
        "                        loss.item(), \n",
        "                        self.lambda_1.item(), \n",
        "                        torch.exp(self.lambda_2).item()\n",
        "                    )\n",
        "                )\n",
        "                \n",
        "        # Backward and optimize\n",
        "        self.optimizer.step(self.loss_func)\n",
        "    \n",
        "    def predict(self, X):\n",
        "        x = torch.tensor(X[:, 0:1], requires_grad=True).float()\n",
        "        t = torch.tensor(X[:, 1:2], requires_grad=True).float()\n",
        "\n",
        "        self.dnn.eval()\n",
        "        u = self.net_u(x, t)\n",
        "        f = self.net_f(x, t)\n",
        "        u = u.detach().cpu().numpy()\n",
        "        f = f.detach().cpu().numpy()\n",
        "        return u, f"
      ],
      "metadata": {
        "id": "uuAxckE0qCgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir(\"/content/drive/\")\n",
        "!ls\n"
      ],
      "metadata": {
        "id": "XnOaTx-JBpVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nu = 0.01/np.pi\n",
        "\n",
        "N_u = 2000\n",
        "layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
        "\n",
        "data = scipy.io.loadmat('MyDrive/colab/RBM_ML/data/burgers_shock.mat')\n",
        "\n",
        "t = data['t'].flatten()[:,None]\n",
        "x = data['x'].flatten()[:,None]\n",
        "Exact = np.real(data['usol']).T\n",
        "\n",
        "X, T = np.meshgrid(x,t)\n",
        "\n",
        "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
        "u_star = Exact.flatten()[:,None]              \n",
        "\n",
        "# Doman bounds\n",
        "lb = X_star.min(0)\n",
        "ub = X_star.max(0) "
      ],
      "metadata": {
        "id": "io6sjJMDqbub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "noise = 0.0            \n",
        "\n",
        "# create training set\n",
        "idx = np.random.choice(X_star.shape[0], N_u, replace=False)\n",
        "X_u_train = X_star[idx,:]\n",
        "u_train = u_star[idx,:]\n",
        "\n",
        "# training\n",
        "model = PhysicsInformedNN(X_u_train, u_train, layers, lb, ub)\n",
        "model.train(0)"
      ],
      "metadata": {
        "id": "PKcFhxy5DBS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluations\n",
        "u_pred, f_pred = model.predict(X_star)\n",
        "\n",
        "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
        "\n",
        "U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
        "\n",
        "lambda_1_value = model.lambda_1.detach().cpu().numpy()\n",
        "lambda_2_value = model.lambda_2.detach().cpu().numpy()\n",
        "lambda_2_value = np.exp(lambda_2_value)\n",
        "\n",
        "error_lambda_1 = np.abs(lambda_1_value - 1.0) * 100\n",
        "error_lambda_2 = np.abs(lambda_2_value - nu) / nu * 100\n",
        "\n",
        "print('Error u: %e' % (error_u))    \n",
        "print('Error l1: %.5f%%' % (error_lambda_1))                             \n",
        "print('Error l2: %.5f%%' % (error_lambda_2))  "
      ],
      "metadata": {
        "id": "ik9ytXKJMtFD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}