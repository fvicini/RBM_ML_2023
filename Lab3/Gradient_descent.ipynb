{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLGyVkeJAhyg"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrdIjjfUMSGu"
      },
      "source": [
        "# **PyTorch & (by hand) gradient descent**\n",
        "\n",
        "**Disclaimer**: large parts of the lab are taken from [Deep Learning with PyTorch: A 60 Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) by [Soumith Chintala](http://soumith.ch/) and lectures material of [Sebastian Goldt](https://datascience.sissa.it/research-unit/12/theory-of-neural-networks)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reRPJPfO4-7q"
      },
      "source": [
        "# **Neural Network: an intuitive definition**\n",
        "\n",
        "Neural networks (NNs) are nested functions over some input data. The functions are defined by weights and biases (in tensor form), the _trainable parameters_.\n",
        "\n",
        "The training of a NN is made by two phases:\n",
        "\n",
        "**Forward Propagation**: the NN makes _a prediction_ and does its best to guess the output by means of the nested functions.\n",
        "\n",
        "**Backward Propagation**: the NN optimizes the parameters trying to lower the _loss_. To do that, it starts from the output and goes _backwards_, computing the derivatives (the _gradients_) of the loss with\n",
        "respect to the trainable parameters and optimizing them by means of gradient descent. If you want to deepen your knowledge, check [this video](https://www.youtube.com/watch?v=tIeHLnjs5U8) out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJ3uvqkNE2ds"
      },
      "source": [
        "# **Gradient Descent by hand**\n",
        "\n",
        "We want to understand if a number is even or odd. To do so, we take a datset of numbers and we apply gradient descent method to a set of data.\n",
        "\n",
        "The gradient descent is the way my Neural Network _optimizes_ the parameters. \n",
        "\n",
        "Imagine to have $m$ data and we want to optimize the parameters of the net minimizing the following _mean square error_ loss:\n",
        "\n",
        "$$\n",
        "L = \\frac{1}{m}\\sum_i^m (y_i - y^{nn}_i)^2\n",
        "$$\n",
        "\n",
        "where $\\displaystyle y^{nn}_i = w \\cdot x_i + b$. The gradient descent algorithm is based on the idea that the _faster_ way to reach a minimum is to follow the negative gradient of the quantity we want to minimize.\n",
        "\n",
        "We need:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w} \\quad \\text{ and } \\quad \\frac{\\partial L}{\\partial b}. \n",
        "$$\n",
        "\n",
        "In the specific case of the mean square error we have:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w} = \\frac{2}{m}\\sum_i^m x_i(y_i -  w \\cdot x_i + b),\n",
        "$$\n",
        "while \n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b} = \\frac{2}{m}\\sum_i^m (y_i -  w \\cdot x_i + b).\n",
        "$$\n",
        "\n",
        "The new parameter are, given a _learning rate_ $\\lambda$:\n",
        "\n",
        "$$\n",
        "w = w - \\lambda \\frac{\\partial L}{\\partial w} \n",
        "\\quad \\text{ and } \\quad \n",
        "b = b - \\lambda \\frac{\\partial L}{\\partial b}.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKDQMLe6E2ds"
      },
      "outputs": [],
      "source": [
        "num_samples = {\"train\": 2000, \"test\": 5000}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYe48xuWE2ds"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGBYxYrfE2ds"
      },
      "outputs": [],
      "source": [
        "modes = [\"train\", \"test\"]\n",
        "\n",
        "#### input = hand-written digits 28x28, output = the number\n",
        "\n",
        "datasets = {\"train\": datasets.MNIST(\"~/datasets/mnist\", \n",
        "                                   train=True,\n",
        "                                   download=True),\n",
        "           \"test\": datasets.MNIST(\"~/datasets/mnist\", \n",
        "                              train=False,\n",
        "                              download=True)}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Dataset**: MNIST images. The MNIST dataset contains 28x28 grayscale images of handwritten digits from 0 to 9. The training set has 60,000 samples, the test set has 10,000 samples. The output is an interger label from 0 to 9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTuTXIcJE2dt"
      },
      "outputs": [],
      "source": [
        "xs = dict()\n",
        "ys = dict()\n",
        "\n",
        "for mode in modes:\n",
        "    xs[mode] = datasets[mode].data[:num_samples[mode]].float()\n",
        "    ys[mode] = datasets[mode].targets[:num_samples[mode]].float()\n",
        "    \n",
        "    mean, std = (torch.mean(xs[mode]), torch.std(xs[mode]))\n",
        "    xs[mode] = (xs[mode] - mean) / std\n",
        "    ys[mode] = 2 * torch.fmod(ys[mode], 2) - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6inp-5dBE2dt"
      },
      "source": [
        "**Question time!!!** What does the above code? Why is it important?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yePCRvI1VFNv"
      },
      "source": [
        "**Answer:** ....\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y9hW5FVE2dt"
      },
      "source": [
        "**Inside the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpoOi93iE2dt"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "d7XLHSJHE2du",
        "outputId": "abffa69e-934f-4ea7-8a78-a4a3e9ab93c4"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 576x432 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(8,6))\n",
        "\n",
        "plt_idx = 0\n",
        "for (n_row, n_col), axes in np.ndenumerate(ax):\n",
        "    axes.imshow(xs[\"train\"][plt_idx])\n",
        "    axes.set_title(\"%d\" % ys[\"train\"][plt_idx])\n",
        "    \n",
        "    axes.set_xticks([])\n",
        "    axes.set_yticks([])\n",
        "    \n",
        "    plt_idx += 1\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSNbY84uE2du"
      },
      "source": [
        "# **Your turn :)**\n",
        "\n",
        "Let us have fun! What about implementing the gradient descent algorithm?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VT81ktcpdYI9"
      },
      "outputs": [],
      "source": [
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7sW3yAQE2du",
        "outputId": "5fa88d07-10c0-4362-eb5f-09852faa912f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2000, 28, 28])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xs[\"train\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeDTkpd7ffMn",
        "outputId": "8c122a4a-1f61-4865-ae8a-153812c438aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.)\n"
          ]
        }
      ],
      "source": [
        "ys[\"train\"].shape\n",
        "print(ys[\"train\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbSKsnXrrn2n"
      },
      "outputs": [],
      "source": [
        "xs[\"train\"] = torch.reshape(xs[\"train\"], (num_samples[\"train\"], xs[\"train\"].shape[1]*xs[\"train\"].shape[1]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yYI-mr49Bqs",
        "outputId": "f290e90d-7675-43c2-b5aa-0c90255ea987"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "784\n",
            "2000\n"
          ]
        }
      ],
      "source": [
        "print(xs[\"train\"].shape[1])\n",
        "print(xs[\"train\"].shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fO53JpqqruuT"
      },
      "outputs": [],
      "source": [
        "xs[\"test\"] = torch.reshape(xs[\"test\"], (num_samples[\"test\"], xs[\"test\"].shape[1]*xs[\"test\"].shape[1]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51LwKFmz9TOo",
        "outputId": "40cbbc3a-13a2-4a31-f1ef-23bd4ccc69f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "784\n",
            "5000\n"
          ]
        }
      ],
      "source": [
        "print(xs[\"test\"].shape[1])\n",
        "print(xs[\"test\"].shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VvUN0gfQeW6"
      },
      "outputs": [],
      "source": [
        "x_train = xs['train']\n",
        "y_train = ys['train']\n",
        "x_test = xs['test']\n",
        "y_test = ys['test']\n",
        "w = torch.rand(x_train.shape[1], requires_grad=True)\n",
        "b = torch.rand(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o9Zp9pa-QCZ"
      },
      "source": [
        "First of all, define the loss function!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AS5BsUwnVZoQ"
      },
      "outputs": [],
      "source": [
        "def  loss(w,b,X,y):\n",
        "    \n",
        "    \n",
        "    m = ...\n",
        "    \n",
        "    predictions = ...\n",
        "    my_loss = ....\n",
        "    return my_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlFdA618-Tn1"
      },
      "source": [
        "Now define the gradient descent algorithm and save how the loss goes for train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUJje9CRV-TT"
      },
      "outputs": [],
      "source": [
        "#### errors dict of \"train\" and \"test\"\n",
        "\n",
        "def gradient_descent(X,y,w,b,errors=None,learning_rate=0.01,epochs=100):\n",
        "    m =  ...\n",
        "    for it in range(epochs):\n",
        "        \n",
        "        w_d = ... # derivative of the loss with respect to w\n",
        "        w = ... # update the weight \n",
        "        b_d = ... # derivative of the loss with respect to b\n",
        "        b = ... # update the bias\n",
        "        \n",
        "        if errors:\n",
        "            for mode in modes:\n",
        "                errors[mode] += [loss(w,b,xs[mode],ys[mode]).item()]\n",
        "              \n",
        "    return w, b, errors\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiNwhQiES3iq"
      },
      "outputs": [],
      "source": [
        "#### initialize the data ####\n",
        "\n",
        "### If you want you can fix the seed...\n",
        "\n",
        "w = torch.rand(x_train.shape[1], requires_grad=True)\n",
        "b = torch.rand(1, requires_grad=True)\n",
        "err = {\"train\": [], \"test\": []}\n",
        "lr =0.0001\n",
        "epochs = 150"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InHKIb60XOe9"
      },
      "outputs": [],
      "source": [
        "w, b, errors = gradient_descent(x_train,y_train,w,b,err,lr,epochs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yBWKIHu_Lob"
      },
      "source": [
        "Let us make a plot!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okI_ekHXE2kF"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.semilogy(errors[\"train\"], label=\"Training\")\n",
        "ax.semilogy(errors[\"test\"], label=\"Testing\")\n",
        "ax.legend()\n",
        "ax.set_title(\"Loss MNIST\\n ($N=%d, \\lambda=%g$)\" % (epochs, lr))\n",
        "ax.set_xlabel(\"step\")\n",
        "ax.set_ylabel(\"mse\")\n",
        "# ax.set_xscale(\"log\")\n",
        "ax.set_ylim(bottom=0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJc6eyB50hxy"
      },
      "source": [
        "# **An introduction to ``torch.autograd``**\n",
        "\n",
        "``torch.autograd`` is the way PyTorch computes derivatives. It is the _automatic differentiation_ tool of the\n",
        "neural network.\n",
        "\n",
        "**How does it work?**: let us create to tensors ``a`` and ``b`` with\n",
        "``requires_grad=True``. The latter option tells ``autograd`` to track every operation on the tensors (useful on back propagation, where you do not want to forget any information). Moreover, let us define the tensot ``T`` as\n",
        "\n",
        "$$ T = a^3 - 2b^2.$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zIMbWhA7H8P"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "a = torch.tensor([2., 3.], requires_grad=True)\n",
        "b = torch.tensor([6., 4.], requires_grad=True)\n",
        "T = a**3 - 2*b**2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsNf40Or7ud7"
      },
      "source": [
        "In this silly example, the tensors ``a`` and ``b`` are the trainable parameters of an NN, while ``T`` is the _loss_ we want to minimize. In a backward training, we want to compute\n",
        "\n",
        "$$\\frac{\\partial T}{\\partial a} = 3a^2 \\text{ and } \\frac{\\partial T}{\\partial b} = -4b.$$\n",
        "\n",
        "In the implementation we _need to explicit_ our goal: i.e. compute derivative of ``T``. To do so, we call ``.backward()`` on ``T``: in this way autograd computes the gradients with respect to the parameters and stores them in the ``.grad`` attribute of the tensor, in our case, ``a.grad`` and ``b.grad``.\n",
        "\n",
        "**Be careful**: we have to pass a ``gradient`` argument in ``T.backward()`` when dealing with vectors. The derivative of ``T`` with respect to ``T`` ia a ``T``-shaped tensor and verifies\n",
        "$$\\frac{dT}{dT} = 1$$\n",
        "\n",
        "The function ``.backward`` applies without arguments to scalar functions (mean square errors, for example). To do so, we can aggragate all the information of ``T`` summing its elements and, only then, calling the ``.backward`` function: ``T.sum().backward()``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQ44SHxA9heE"
      },
      "outputs": [],
      "source": [
        "external_grad = torch.tensor([1., 1.])\n",
        "T.backward(gradient=external_grad)\n",
        "# T.sum().backward()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYlGtQGTAZTX"
      },
      "outputs": [],
      "source": [
        "# check if collected gradients are correct\n",
        "print(3*a**2 == a.grad)\n",
        "print(-4*b == b.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMRP3lHpBeyp"
      },
      "source": [
        "# **Your turn :)**\n",
        "\n",
        "Can you implement gradient descend with ``.autograd``?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOloBNidEfJm"
      },
      "outputs": [],
      "source": [
        "def  loss(w,b,X,y):\n",
        "    \n",
        "    \n",
        "    m = ...\n",
        "    \n",
        "    predictions = ...\n",
        "    my_loss = ...\n",
        "    return my_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABJAwkxhFmlU"
      },
      "outputs": [],
      "source": [
        "def auto_gradient_descent(X,y,w,b,errors=None,learning_rate=0.01,epochs=100):\n",
        "    m =  ...\n",
        "    for it in range(epochs):\n",
        "        my_loss = loss(w,b,X,y)\n",
        "        my_loss.backward()\n",
        "        \n",
        "        with torch.no_grad(): # do note trace once again \n",
        "            w = ... #update w\n",
        "            b = ... # update p\n",
        "            w.requires_grad_(True)\n",
        "            b.requires_grad_(True) \n",
        "\n",
        "        w.grad = None\n",
        "        b.grad = None\n",
        "    \n",
        "        with torch.no_grad():\n",
        "            if errors:\n",
        "                for mode in modes:\n",
        "                    errors[mode] += [loss(w,b,xs[mode],ys[mode]).item()]\n",
        "\n",
        "    return w, b, errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzZ9RbdZSnjG"
      },
      "outputs": [],
      "source": [
        "#### initialize the data ####\n",
        "w = torch.rand(x_train.shape[1], requires_grad=True)\n",
        "b = torch.rand(1, requires_grad=True)\n",
        "err = {\"train\": [], \"test\": []}\n",
        "lr =0.001\n",
        "epochs = 500\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pm1DNcysJ0s-"
      },
      "outputs": [],
      "source": [
        "w, b, errors = auto_gradient_descent(x_train,y_train,w,b,err,lr,epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9O1SgmR5J7Tb"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.plot(errors[\"train\"], label=\"Training\")\n",
        "ax.plot(errors[\"test\"], label=\"Testing\")\n",
        "ax.legend()\n",
        "ax.set_title(\"Loss on MNIST\\n ($N=%d, \\lambda=%g$)\" % (epochs, lr))\n",
        "ax.set_xlabel(\"step\")\n",
        "ax.set_ylabel(\"mse\")\n",
        "ax.set_yscale(\"log\")\n",
        "ax.set_ylim(bottom=0)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
