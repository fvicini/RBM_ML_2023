{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrdIjjfUMSGu"
      },
      "source": [
        "# **PyTorch & (by hand) gradient descent**\n",
        "\n",
        "**Disclaimer**: large parts of the lab are taken from [Deep Learning with PyTorch: A 60 Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) by [Soumith Chintala](http://soumith.ch/) and lectures material of [Sebastian Goldt](https://datascience.sissa.it/research-unit/12/theory-of-neural-networks)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reRPJPfO4-7q"
      },
      "source": [
        "# **Neural Network: an intuitive definition**\n",
        "\n",
        "Neural networks (NNs) are nested functions over some input data. The functions are defined by weights and biases (in tensor form), the _trainable parameters_.\n",
        "\n",
        "The training of a NN is made by two phases:\n",
        "\n",
        "**Forward Propagation**: the NN makes _a prediction_ and does its best to guess the output by means of the nested functions.\n",
        "\n",
        "**Backward Propagation**: the NN optimizes the parameters trying to lower the _loss_. To do that, it starts from the output and goes _backwards_, computing the derivatives (the _gradients_) of the loss with\n",
        "respect to the trainable parameters and optimizing them by means of gradient descent. If you want to deepen your knowledge, check [this video](https://www.youtube.com/watch?v=tIeHLnjs5U8) out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJ3uvqkNE2ds"
      },
      "source": [
        "# **Gradient Descent by hand**\n",
        "\n",
        "We want to understand if a number is even or odd. To do so, we take a datset of numbers and we apply gradient descent method to a set of data.\n",
        "\n",
        "The gradient descent is the way my Neural Network _optimizes_ the parameters. \n",
        "\n",
        "Imagine to have $m$ data and we want to optimize the parameters of the net minimizing the following _mean square error_ loss:\n",
        "\n",
        "$$\n",
        "L = \\frac{1}{m}\\sum_i^m (y^{nn}_i - y_i)^2\n",
        "$$\n",
        "\n",
        "where $\\displaystyle y^{nn}_i = w \\cdot x_i + b$. The gradient descent algorithm is based on the idea that the _faster_ way to reach a minimum is to follow the negative gradient of the quantity we want to minimize.\n",
        "\n",
        "We need:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w} \\quad \\text{ and } \\quad \\frac{\\partial L}{\\partial b}. \n",
        "$$\n",
        "\n",
        "In the specific case of the mean square error we have:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w} = \\frac{2}{m}\\sum_i^m x_i( w \\cdot x_i + b - y_i),\n",
        "$$\n",
        "while \n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b} = \\frac{2}{m}\\sum_i^m ( w \\cdot x_i + b - y_i).\n",
        "$$\n",
        "\n",
        "The new parameter are, given a _learning rate_ $\\lambda$:\n",
        "\n",
        "$$\n",
        "w = w - \\lambda \\frac{\\partial L}{\\partial w} \n",
        "\\quad \\text{ and } \\quad \n",
        "b = b - \\lambda \\frac{\\partial L}{\\partial b}.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKDQMLe6E2ds"
      },
      "outputs": [],
      "source": [
        "num_samples = {\"train\": 2000, \"test\": 5000}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYe48xuWE2ds"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGBYxYrfE2ds"
      },
      "outputs": [],
      "source": [
        "modes = [\"train\", \"test\"]\n",
        "\n",
        "#### input = hand-written digits 28x28, output = the number\n",
        "\n",
        "datasets = {\"train\": datasets.MNIST(\"~/datasets/mnist\", \n",
        "                                   train=True,\n",
        "                                   download=True),\n",
        "           \"test\": datasets.MNIST(\"~/datasets/mnist\", \n",
        "                              train=False,\n",
        "                              download=True)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUTji9u8frbW"
      },
      "source": [
        "**Dataset**: MNIST images. The MNIST dataset contains 28x28 grayscale images of handwritten digits from 0 to 9. The training set has 60,000 samples, the test set has 10,000 samples. The output is an interger label from 0 to 9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTuTXIcJE2dt"
      },
      "outputs": [],
      "source": [
        "xs = dict()\n",
        "ys = dict()\n",
        "\n",
        "for mode in modes:\n",
        "    xs[mode] = datasets[mode].data[:num_samples[mode]].float()\n",
        "    ys[mode] = datasets[mode].targets[:num_samples[mode]].float()\n",
        "    \n",
        "    mean, std = (torch.mean(xs[mode]), torch.std(xs[mode]))\n",
        "    xs[mode] = (xs[mode] - mean) / std\n",
        "    ys[mode] = 2 * torch.fmod(ys[mode], 2) - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6inp-5dBE2dt"
      },
      "source": [
        "**Question time!!!** What does the above code? Why is it important?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yePCRvI1VFNv"
      },
      "source": [
        "**Answer:** it normilizes the data (useful in many application) and changes the output from a number from 0 to 9 to the \"odd class\" (1) and \"even class\" (-1).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y9hW5FVE2dt"
      },
      "source": [
        "**Inside the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpoOi93iE2dt"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7XLHSJHE2du"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(8,6))\n",
        "\n",
        "plt_idx = 0\n",
        "for (n_row, n_col), axes in np.ndenumerate(ax):\n",
        "    axes.imshow(xs[\"train\"][plt_idx])\n",
        "    axes.set_title(\"%d\" % ys[\"train\"][plt_idx])\n",
        "    \n",
        "    axes.set_xticks([])\n",
        "    axes.set_yticks([])\n",
        "    \n",
        "    plt_idx += 1\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSNbY84uE2du"
      },
      "source": [
        "# **Your turn :)**\n",
        "\n",
        "Let us have fun! What about implementing the gradient descent algorithm?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VT81ktcpdYI9"
      },
      "outputs": [],
      "source": [
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7sW3yAQE2du"
      },
      "outputs": [],
      "source": [
        "xs[\"train\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeDTkpd7ffMn"
      },
      "outputs": [],
      "source": [
        "print(ys[\"train\"].shape)\n",
        "print(ys[\"train\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbSKsnXrrn2n"
      },
      "outputs": [],
      "source": [
        "xs[\"train\"] = torch.reshape(xs[\"train\"], (num_samples[\"train\"], xs[\"train\"].shape[1]*xs[\"train\"].shape[1]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yYI-mr49Bqs"
      },
      "outputs": [],
      "source": [
        "print(xs[\"train\"].shape[1])\n",
        "print(xs[\"train\"].shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fO53JpqqruuT"
      },
      "outputs": [],
      "source": [
        "xs[\"test\"] = torch.reshape(xs[\"test\"], (num_samples[\"test\"], xs[\"test\"].shape[1]*xs[\"test\"].shape[1]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51LwKFmz9TOo"
      },
      "outputs": [],
      "source": [
        "print(xs[\"test\"].shape[1])\n",
        "print(xs[\"test\"].shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VvUN0gfQeW6"
      },
      "outputs": [],
      "source": [
        "x_train = xs['train']\n",
        "y_train = ys['train']\n",
        "x_test = xs['test']\n",
        "y_test = ys['test']\n",
        "w = torch.rand(x_train.shape[1], requires_grad=True)\n",
        "b = torch.rand(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o9Zp9pa-QCZ"
      },
      "source": [
        "First of all, define the loss function!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AS5BsUwnVZoQ"
      },
      "outputs": [],
      "source": [
        "def  loss(w,b,X,y):\n",
        "    \n",
        "    \n",
        "    m = X.shape[0]\n",
        "    \n",
        "    predictions = w @ X.T + b\n",
        "    my_loss = (1/m)* torch.sum((predictions - y)**2)\n",
        "    return my_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlFdA618-Tn1"
      },
      "source": [
        "Now define the gradient descent algorithm and save how the loss goes for train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUJje9CRV-TT"
      },
      "outputs": [],
      "source": [
        "#### errors dict of \"train\" and \"test\"\n",
        "\n",
        "def gradient_descent(X,y,w,b,errors=None,learning_rate=0.01,epochs=100):\n",
        "    m =  X.shape[0]\n",
        "    for it in range(epochs):\n",
        "        \n",
        "        prediction = w @ X.T + b\n",
        "        w_d =  (2/m)*(prediction - y) @ X # derivative of the loss with respect to w\n",
        "        w = w - learning_rate*w_d # update the weight \n",
        "        b_d = (2/m)*torch.sum((prediction - y)) # derivative of the loss with respect to b\n",
        "        b = b - learning_rate*b_d # update the bias\n",
        "        \n",
        "        if errors:\n",
        "            for mode in modes:\n",
        "                errors[mode] += [loss(w,b,xs[mode],ys[mode]).item()]\n",
        "              \n",
        "    return w, b, errors\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiNwhQiES3iq"
      },
      "outputs": [],
      "source": [
        "#### initialize the data ####\n",
        "\n",
        "### If you want you can fix the seed...\n",
        "\n",
        "w = torch.rand(x_train.shape[1], requires_grad=True)\n",
        "b = torch.rand(1, requires_grad=True)\n",
        "err = {\"train\": [], \"test\": []}\n",
        "lr =0.0001\n",
        "epochs = 150"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InHKIb60XOe9"
      },
      "outputs": [],
      "source": [
        "w, b, errors = gradient_descent(x_train,y_train,w,b,err,lr,epochs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yBWKIHu_Lob"
      },
      "source": [
        "Let us make a plot!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okI_ekHXE2kF"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.semilogy(errors[\"train\"], label=\"Training\")\n",
        "ax.semilogy(errors[\"test\"], label=\"Testing\")\n",
        "ax.legend()\n",
        "ax.set_title(\"Loss MNIST\\n ($N=%d, \\lambda=%g$)\" % (epochs, lr))\n",
        "ax.set_xlabel(\"step\")\n",
        "ax.set_ylabel(\"mse\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJc6eyB50hxy"
      },
      "source": [
        "# **An introduction to ``torch.autograd``**\n",
        "\n",
        "``torch.autograd`` is the way PyTorch computes derivatives. It is the _automatic differentiation_ tool of the\n",
        "neural network.\n",
        "\n",
        "**How does it work?**: let us create to tensors ``a`` and ``b`` with\n",
        "``requires_grad=True``. The latter option tells ``autograd`` to track every operation on the tensors (useful on back propagation, where you do not want to forget any information). Moreover, let us define the tensot ``T`` as\n",
        "\n",
        "$$ T = a^3 - 2b^2.$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zIMbWhA7H8P"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "a = torch.tensor([2., 3.], requires_grad=True)\n",
        "b = torch.tensor([6., 4.], requires_grad=True)\n",
        "T = a**3 - 2*b**2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsNf40Or7ud7"
      },
      "source": [
        "In this silly example, the tensors ``a`` and ``b`` are the trainable parameters of an NN, while ``T`` is the _loss_ we want to minimize. In a backward training, we want to compute\n",
        "\n",
        "$$\\frac{\\partial T}{\\partial a} = 3a^2 \\text{ and } \\frac{\\partial T}{\\partial b} = -4b.$$\n",
        "\n",
        "In the implementation we _need to explicit_ our goal: i.e. compute derivative of ``T``. To do so, we call ``.backward()`` on ``T``: in this way autograd computes the gradients with respect to the parameters and stores them in the ``.grad`` attribute of the tensor, in our case, ``a.grad`` and ``b.grad``.\n",
        "\n",
        "**Be careful**: we have to pass a ``gradient`` argument in ``T.backward()`` when dealing with vectors. The derivative of ``T`` with respect to ``T`` ia a ``T``-shaped tensor and verifies\n",
        "$$\\frac{dT}{dT} = 1$$\n",
        "\n",
        "The function ``.backward`` applies without arguments to scalar functions (mean square errors, for example). To do so, we can aggragate all the information of ``T`` summing its elements and, only then, calling the ``.backward`` function: ``T.sum().backward()``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQ44SHxA9heE"
      },
      "outputs": [],
      "source": [
        "external_grad = torch.tensor([1., 1.])\n",
        "T.backward(gradient=external_grad)\n",
        "# T.sum().backward() \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYlGtQGTAZTX"
      },
      "outputs": [],
      "source": [
        "# check if collected gradients are correct\n",
        "print(3*a**2 == a.grad)\n",
        "print(-4*b == b.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMRP3lHpBeyp"
      },
      "source": [
        "# **Your turn :)**\n",
        "\n",
        "Can you implement gradient descend with ``.autograd``?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOloBNidEfJm"
      },
      "outputs": [],
      "source": [
        "def  loss(w,b,X,y):\n",
        "    \n",
        "    \n",
        "    predictions = w @ X.T + b\n",
        "    my_loss = torch.mean((predictions - y)**2)\n",
        "    \n",
        "    return my_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABJAwkxhFmlU"
      },
      "outputs": [],
      "source": [
        "def auto_gradient_descent(X,y,w,b,errors=None,learning_rate=0.01,epochs=100):\n",
        "    \n",
        "    for it in range(epochs):\n",
        "        my_loss = loss(w,b,X,y)\n",
        "        my_loss.backward()\n",
        "        \n",
        "        with torch.no_grad(): # do note trace once again \n",
        "            w = w - learning_rate*w.grad #update w\n",
        "            b = b - learning_rate*b.grad # update b\n",
        "            w.requires_grad_(True)\n",
        "            b.requires_grad_(True) \n",
        "\n",
        "        w.grad = None\n",
        "        b.grad = None\n",
        "    \n",
        "        with torch.no_grad():\n",
        "            if errors:\n",
        "                for mode in modes:\n",
        "                    errors[mode] += [loss(w,b,xs[mode],ys[mode]).item()]\n",
        "\n",
        "    return w, b, errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzZ9RbdZSnjG"
      },
      "outputs": [],
      "source": [
        "#### initialize the data ####\n",
        "w = torch.rand(x_train.shape[1], requires_grad=True)\n",
        "b = torch.rand(1, requires_grad=True)\n",
        "err = {\"train\": [], \"test\": []}\n",
        "lr =0.001\n",
        "epochs = 500\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pm1DNcysJ0s-"
      },
      "outputs": [],
      "source": [
        "w, b, errors = auto_gradient_descent(x_train,y_train,w,b,err,lr,epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9O1SgmR5J7Tb"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.plot(errors[\"train\"], label=\"Training\")\n",
        "ax.plot(errors[\"test\"], label=\"Testing\")\n",
        "ax.legend()\n",
        "ax.set_title(\"Loss on MNIST\\n ($N=%d, \\lambda=%g$)\" % (epochs, lr))\n",
        "ax.set_xlabel(\"step\")\n",
        "ax.set_ylabel(\"mse\")\n",
        "ax.set_yscale(\"log\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}